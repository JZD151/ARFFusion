# ARFFusion

[![LICENSE](https://img.shields.io/badge/License-MIT-green)](https://github.com/lok-18/IGNet/blob/master/LICENSE)

### Adversarially Robust Fourier-Aware Multimodal Medical Image Fusion for LSCI
### â€¼ï¸*Environment* 

```shell
conda env create -f environment/environment.yml
conda activate LSCI
```

### ðŸ“‘*Dataset setting*
> We give several test image pairs as examples in [[*Harvard Medical website*]](http://www.med.harvard.edu/AANLIB/home.html), respectively.
>
> Moreover, you can set your own test datasets of different modalities under ```./medicine_test/...```, like:   
> ```
> medicine_test
> â”œâ”€â”€ mri_ct
> â”‚   â”œâ”€â”€ mri
> â”‚   â”‚   â”œâ”€â”€ 001.png
> â”‚   â”‚   â”œâ”€â”€ 002.png
> â”‚   â”‚   â””â”€â”€ ...
> â”‚   â”œâ”€â”€ oth
> â”‚   â”‚   â”œâ”€â”€ 001.png
> â”‚   â”‚   â”œâ”€â”€ 002.png
> â”‚   â”‚   â””â”€â”€ ...
> â”‚   â””â”€â”€ Pseudo_label
> â”‚       â”œâ”€â”€ 001.png
> â”‚       â”œâ”€â”€ 002.png
> â”‚       â””â”€â”€ ...
> â””â”€â”€ mri_pet
>     â”œâ”€â”€ mri
>     â”‚   â”œâ”€â”€ 175.png
>     â”‚   â”œâ”€â”€ 176.png
>     â”‚   â””â”€â”€ ...
>     â”œâ”€â”€ oth
>     â”‚   â”œâ”€â”€ 175.png
>     â”‚   â”œâ”€â”€ 176.png
>     â”‚   â””â”€â”€ ...
>     â””â”€â”€ Pseudo_label
>         â”œâ”€â”€ 175.png
>         â”œâ”€â”€ 176.png
>         â””â”€â”€ ...
> ```
>
> The configuration of the training dataset is similar to the aforementioned format.

### ðŸ–¥ï¸*Test*
> The pre-trained model `model.pth` has given in [[*Google Drive*\]](https://drive.google.com/file/d/1X1QneHZeQNwpDKhDWbGICBV4h79EvP4e/view?usp=drive_link) and [[*Baidu Yun*\]](https://pan.baidu.com/s/1UDEu_Mwkl0G8TnOO8KSLlQ?pwd=968k).
>
> Please put ```model.pth``` into ```./checkpoint/``` and run ```test_robust.py``` to get fused results. You can check them in:
> ```
> test
> â””â”€â”€ results
>     â”œâ”€â”€ mri_ct
>     â”‚   â”œâ”€â”€ advmri
>     â”‚   â”‚   â”œâ”€â”€ 001.png
>     â”‚   â”‚   â”œâ”€â”€ 002.png
>     â”‚   â”‚   â””â”€â”€ ...
>     â”‚   â”œâ”€â”€ advoth
>     â”‚   â”‚   â”œâ”€â”€ 001.png
>     â”‚   â”‚   â”œâ”€â”€ 002.png
>     â”‚   â”‚   â””â”€â”€ ...
>     â”‚   â””â”€â”€ fused
>     â”‚       â”œâ”€â”€ 001.png
>     â”‚       â”œâ”€â”€ 002.png
>     â”‚       â””â”€â”€ ...
>     â””â”€â”€ mri_pet
>         â”œâ”€â”€ advmri
>         â”‚   â”œâ”€â”€ 175.png
>         â”‚   â”œâ”€â”€ 176.png
>         â”‚   â””â”€â”€ ...
>         â”œâ”€â”€ advoth
>         â”‚   â”œâ”€â”€ 175.png
>         â”‚   â”œâ”€â”€ 176.png
>         â”‚   â””â”€â”€ ...
>         â””â”€â”€ fused
>             â”œâ”€â”€ 175.png
>             â”œâ”€â”€ 176.png
>             â””â”€â”€ ...

### âŒ›*Train*
> You can also utilize your own data to train a new robust fusion model with: 
> ```shell
> python train_robust.py
> ```

### ðŸ“¬*Contact*
> If you have any questions, please create an issue or email to archerv2@mail.nwpu.edu.cn.
